# src/codex_veritas/analysis/semantic_layer.py

"""
Tier 2: Semantic Layer Builder.

This module takes the structural code graph generated by Tier 1 (ast_parser)
and enriches it with semantic meaning. It reads the code content for each
meaningful node in the graph (functions, classes, methods), generates a dense
vector embedding for it using a sentence-transformer model, and stores these
embeddings in a ChromaDB vector database.

This semantic index allows for powerful similarity searches, enabling the AI agent
to find relevant code snippets based on natural language queries, even if the
query doesn't use the exact keywords present in the code.
"""

import os
import json
import chromadb
from chromadb.utils import embedding_functions
from typing import List, Dict, Any
from pathlib import Path
import shutil

# --- Configuration ---
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
BATCH_SIZE = 100

# --- Helper function to prepare code chunks ---

def prepare_code_chunks_from_graph(graph_path: Path, repo_path: Path) -> List[Dict[str, Any]]:
    """
    Reads the code_graph.json, gets the full code for each node,
    and prepares it for batch embedding.
    """
    print("--- preparing code chunks from graph for embedding... ---")
    try:
        with open(graph_path, 'r', encoding='utf-8') as f:
            code_graph = json.load(f)
    except FileNotFoundError:
        print(f"--- ‚ùå ERROR: The code graph file was not found at {graph_path}")
        print("--- Please run the tier1_ast_parser.py script first.")
        return []

    chunks_to_process = []
    
    for node in code_graph.get('nodes', []):
        if node['type'] in ['function', 'class', 'method']:
            try:
                absolute_file_path = repo_path / node['file']
                
                # For simplicity, we embed the entire file's content for the node.
                # A more advanced implementation might extract the specific function/class block.
                with open(absolute_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    code_string = f.read()

                if code_string:
                    chunks_to_process.append({
                        "id": node['id'],
                        "code_string": code_string,
                        "file": node['file'],
                        "type": node['type']
                    })
            except FileNotFoundError:
                print(f"  - ‚ö†Ô∏è Warning: Could not find file {node['file']} for node {node['id']}. Skipping.")
                continue
    
    unique_chunks = {chunk['id']: chunk for chunk in chunks_to_process}
    final_chunks = list(unique_chunks.values())
    
    print(f"--- ‚úÖ Prepared {len(final_chunks)} unique code chunks for embedding. ---")
    return final_chunks


# --- Main Execution Function ---
def build_semantic_layer(graph_path: Path, repo_path: Path, db_path: Path, collection_name: str):
    """
    Main function to run the semantic layer pipeline.
    """
    print("\n--- üü¢ Starting Tier 2: Building Code-Aware Semantic Layer ---")
    
    # --- ChromaDB Setup ---
    print("--- üß† Initializing code-aware embedding model... ---")
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    # --- FIXED HERE: Delete the old database directory for a clean start ---
    if db_path.exists():
        print(f"--- üóëÔ∏è Deleting old ChromaDB database at {db_path}. ---")
        shutil.rmtree(db_path)

    client = chromadb.PersistentClient(path=str(db_path))

    collection = client.create_collection(
        name=collection_name,
        embedding_function=sentence_transformer_ef,
        metadata={"hnsw:space": "cosine"}
    )
    print(f"--- ‚ú® Created new ChromaDB collection '{collection_name}'. ---")

    all_chunks = prepare_code_chunks_from_graph(graph_path, repo_path)
    if not all_chunks:
        print("--- No code chunks to process. Exiting. ---")
        return

    for i in range(0, len(all_chunks), BATCH_SIZE):
        batch = all_chunks[i:i + BATCH_SIZE]
        batch_num = (i // BATCH_SIZE) + 1
        total_batches = (len(all_chunks) + BATCH_SIZE - 1) // BATCH_SIZE
        
        print(f"\n--- üîÑ Processing batch {batch_num}/{total_batches}... ---")
        
        ids_list = [chunk['id'] for chunk in batch]
        docs_list = [chunk['code_string'] for chunk in batch]
        meta_list = [
            {"file": chunk['file'], "type": chunk['type'], "code": chunk['code_string']} 
            for chunk in batch
        ]

        try:
            collection.add(
                ids=ids_list,
                documents=docs_list,
                metadatas=meta_list
            )
            print(f"  - ‚úÖ Successfully embedded and stored {len(ids_list)} items.")
        except Exception as e:
            print(f"  - ‚ùå An error occurred during ChromaDB insertion: {e}")

    print(f"\n--- üéâ Tier 2: Semantic Layer Build Complete ---")
    print(f"--- Vector database is now up-to-date at: {db_path} ---")

